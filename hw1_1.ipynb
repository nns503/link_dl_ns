{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be22fd4e-5b4f-4157-a808-8b46ff61e0a7",
   "metadata": {},
   "source": [
    "<h1>a_tensor_initialization.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dc439a-2206-4ac1-b438-ba7743e3dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ed9e90-0cb7-4c7a-90d7-2bea1cb4e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu') # [1,2,3]을 텐서로 생성, device는 텐서를 CPU에 할당하는 것을 의미함\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e943f367-8c75-4c22-99fa-d53e07c224ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 1\n"
     ]
    }
   ],
   "source": [
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu() # t1 텐서를 CPU 메모리로 이동시킴, 이미 CPU에 있을 경우 아무런 변환 없이 반환\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c15007d-1318-4ad9-81f5-957e2260799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # >>> torch.int64 # 텐서의 데이터 타입 확인\n",
    "print(t2.device)  # >>> cpu \n",
    "print(t2.requires_grad)  # >>> False # 기울기 보유 여뷰 -> 학습을 미분으로 하기 때문에 필요함\n",
    "print(t2.size())  # torch.Size([3]) # 인자를 통해 차원의 size 조회 가능\n",
    "print(t2.shape)  # torch.Size([3]) # 인덱스를 통해 차원의 size를 조회 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c4c60e-14c9-47a4-8879-7495a7f07005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 2\n"
     ]
    }
   ],
   "source": [
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu() # 만약 GPU 사용이 가능하다면 위 주석 메소드로 cuda 환경에서 사용 가능\n",
    "print(\"#\" * 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa72131c-a079-46e1-88fb-fc24900a2055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0 # 실수는 차원이 없음\n",
    "print(a1.shape, a1.ndim) # shape은 shape를 출력하고, ndim은 차원을 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c2f4846-7ca2-479f-8ae8-33ba4563115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1]) 1\n"
     ]
    }
   ],
   "source": [
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6663c756-3405-4b53-828d-1604716f3a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5]) 1\n"
     ]
    }
   ],
   "source": [
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eea07f7-d8aa-49ac-baa5-2172493ff23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1]) 2\n"
     ]
    }
   ],
   "source": [
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb7a56a-aca3-4b9a-a983-f3f9c1878e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) 2\n"
     ]
    }
   ],
   "source": [
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44fdeec0-89ca-459a-b15c-0667daf98487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1]) 3\n"
     ]
    }
   ],
   "source": [
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc4931e-c3db-44f1-931d-254d0c2fd795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 1]) 4\n"
     ]
    }
   ],
   "source": [
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da4d2124-05e0-49e9-8d89-c9b6fc82031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 3]) 4\n"
     ]
    }
   ],
   "source": [
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "333cf0bd-8e90-4492-a697-9893a9540eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 3, 1]) 5\n"
     ]
    }
   ],
   "source": [
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "930b97da-3cf9-4f4f-ba6d-5c77809785ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5]) 2\n"
     ]
    }
   ],
   "source": [
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3334f875-8b51-4aab-9760-4ad7ff4b8fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 5]) 3\n"
     ]
    }
   ],
   "source": [
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18e65409-94f4-4450-9da9-a2959214bcb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a11 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([                 \u001b[38;5;66;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m      3\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m      4\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m      5\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m      6\u001b[0m ])\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "# 각 차원에서 일관된 크기가 아니므로 오류 발생\n",
    "# 괄호의 중첩의 수가 텐서의 rank가 되며, 쉼표 수 + 1이 사이즈가 됨(만약 쉼표 이후 값이 없는 쉼표가 존재할 경우 쉼표 수 = 사이즈)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8498dfa-c841-478d-b604-ad2ade3c3fcb",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "기본적인 Shape이나 차원에 대해서 알게 되었으며, 텐서의 정보를 확인하는 기본적인 함수뿐만 아니라\n",
    "텐서의 형태를 보고 차원이나 사이즈를 파악하는 방법을 많은 예제를 통하여 연습하고 학습함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd1512-d9ad-43cc-bf63-b27306c31e8c",
   "metadata": {},
   "source": [
    "<h1>b_tensor_initialization_copy.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8425563d-c195-4b55-9f94-bccc969965e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c27460d-6514-4a5e-abb1-2b99d32f5c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "# 리스트를 생성하고 텐서로 변환함\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1) #torch.Tensor는 기본으로 float32 타입의 텐서를 만듬\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2) #torch.Tensor는 기본으로 입력 데이터의 데이터 타입을 유지함, 데이터 타입 명시 가능\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3) #텐서 변환 시 가능하면 복사가 아닌 참조를 함\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "#t1, t2는 복사를 하였음으로 기존 리스트의 값을 변경하였음에도 값이 수정이 되자 않음\n",
    "#numpy가 아닌 파이썬의 리스트이므로 as_tensor을 사용하였음에도 참조가 아닌 복사가 됨\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2191c396-80d4-4740-9a33-e41df734c155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Numpy 배열로 텐서를 생성함\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4) \n",
    "print(t5) \n",
    "print(t6) # numpy array는 as_tensor 시 참조되므로 첫번째 값이 100으로 변경됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c2994-3b1c-45a0-9687-088eb1299fab",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "tensor와 Tensor 함수를 통하여 텐서를 만드는 방법에 대해서 학습하고\n",
    "as_tensor을 통해 참조를 통해 텐서를 만들어 메모리를 절약하는 방법에 대해서 학습함\n",
    "특히 numpy array를 사용하여야 텐서를 참조하여 생성할 수 있다는 것이 중요한 내용이라고 생각함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e57ff-7702-4b85-8531-893de58e557b",
   "metadata": {},
   "source": [
    "<h1>c_tensor_initialization_constant_values.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff5bfa1-60b5-4428-87b3-ab65cccff17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca8a7816-886e-43c7-b68a-b5fc22f12661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(size=(5,))  # or torch.ones(5) size 크기의 1로 채워진 텐서 생성\n",
    "t1_like = torch.ones_like(input=t1) # input 텐서와 동일한 크기와 속성을 가진 1로 채워진 텐서 생성\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f32e51b-c870-4e4c-8844-0abe305ddc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6) size 크기의 0으로 채워진 텐서 생성\n",
    "t2_like = torch.zeros_like(input=t2) # input 텐서와 동일한 크기와 속성을 가진 0로 채워진 텐서 생성\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c4d84ea-eef6-4d7f-9b8d-c202f1fee4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0.])\n",
      "tensor([1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4) size 크기의 초기화되지 않은 텐서 생성\n",
    "t3_like = torch.empty_like(input=t3) # input 텐서와 동일한 크기와 속성을 가진 초기화 안된 텐서 생성\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "587bb3a9-997e-42e1-92f2-1c7d9cb26d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.eye(n=3) # 단위 행렬을 생성하는 함수\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d0de9-709e-4123-bbab-3b0ed546f4fd",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "기존 리스트나 numpy array 없이 텐서를 만드는 방법에 대해서 학습할 수 있었음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a660d9b-ff28-4d65-9ff0-6ed3d4b8cf63",
   "metadata": {},
   "source": [
    "<h1>d_tensor_initialization_random_values.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec97db6f-d449-40e3-a873-f2a95d983b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7afa2e90-62c8-47e7-90bb-64c13776cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 18]])\n",
      "tensor([[0.4361, 0.1371, 0.6101]])\n",
      "tensor([[-2.2053,  0.6613, -0.7571]])\n",
      "tensor([[11.2078,  8.9235],\n",
      "        [10.1925,  9.1012],\n",
      "        [ 9.4240, 10.2642]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randint(low=10, high=20, size=(1, 2)) # low, high 내 랜덤한 정수 값의 텐서 생성 \n",
    "print(t1)\n",
    "t2 = torch.rand(size=(1, 3)) # 균등 분포에서 랜덤한 정수 값의 텐서 생성\n",
    "print(t2)\n",
    "t3 = torch.randn(size=(1, 3)) # 정규 분포 형태의 랜덤한 정수 값의 텐서 생성\n",
    "print(t3)\n",
    "# mean은 평균 값, std는 표준편차에 따른 정규 분포 값 생성\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2)) \n",
    "print(t4)\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3) # start부터 end까지 일정하게 나뉜 텐서를 생성\n",
    "print(t5)\n",
    "t6 = torch.arange(5) # 범위 내 연속적인 정수 배열 생성\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c2bf275-be89-4042-82f4-cceb906ee425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729) # 토치의 랜덤 시드를 설정함, 시드에 따라 시작 시 동일한 결과 순서가 나옴\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18d6c9-dd22-4763-8a2f-805e80faa1f3",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "랜덤한 수를 만드는 방법에 대해서 학습함\n",
    "특히 시드를 활용하여 랜덤한 수를 생성하지만, 그 랜덤한 수를 반복해서 활용할 수 있다는 것에 대해서 알게 되었음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad523b1-3417-426f-8602-ab68a4346e51",
   "metadata": {},
   "source": [
    "<h1>e_tensor_type_conversion.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbbb07ed-8c1a-41c3-9cc7-50d0144df8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68fe315b-1c7c-470f-9045-439659ae5cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3)) # (2, 3) 크기의 값이 1인 텐서 생성 기본 타입은 float32\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16) # int16 데이터 타입 (2, 3) 크기의 값이 1인 텐서 생성 \n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20. # (2, 3) 크기인 0과 1 사이의 균등 분포의 실수로 채워진 텐서 생성\n",
    "print(c)\n",
    "\n",
    "d = b.to(torch.int32) # 주어진 타입으로 텐서의 데이터 타입을 변환함\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53117981-8a7e-4309-a458-45a0d5970ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.int16\n"
     ]
    }
   ],
   "source": [
    "# ones 함수는 1로 채워진 텐서이며, zeros 함수는 0으로 채워진 텐서임 기본 타입은 float32\n",
    "double_d = torch.ones(10, 2, dtype=torch.double) \n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double) # 0으로 채워진 텐서 생성 후 타입 변경\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double) # 데이터 타입이 변경됨\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3a0a890-b25d-4e74-9363-57146824e8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b05977-680c-45ba-8266-f179c428df8b",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "텐서의 형변환에 대해서 학습함\n",
    "생성 시 인자를 활용하는 방법, 함수를 이용하는 방법, to 함수를 통하여 형변환하는 방법에 대해서 학습함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4b5c4-6d77-492a-9bcd-ce19d7c67779",
   "metadata": {},
   "source": [
    "<h1>f_tensor_operations.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0596e83e-018c-4dcb-bb8d-3e2322bcea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e9035dd-3129-4a37-93a5-ad7a757cc42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2) # 두 텐서를 덧셈을 함\n",
    "t4 = t1 + t2 # add 함수와 동일한 덧셈 동작을 함\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "print(\"#\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f2e169f-5ca6-43ad-9f96-9f454c5fafca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.sub(t1, t2) # 두 텐서를 뺄셈을 함\n",
    "t6 = t1 - t2 # sub 함수와 동일한 뺄셈 동작을 함\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "062a172c-0202-4777-8a38-75ffbc78e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.mul(t1, t2) # 두 텐서를 곱셈함\n",
    "t8 = t1 * t2 # mul 함수와 동일한 곱셈 동작을 함\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6728199-0748-4e92-8333-27437edd2103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.div(t1, t2) # 두 텐서를 나눗셈함\n",
    "t10 = t1 / t2 # div 함수와 동일한 나눗셈 동작을 함\n",
    "print(t9)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9226c-d8a3-4007-baf2-5e50ac6edd5e",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "텐서의 기본 연산에 대해서 학습함\n",
    "특히 행렬의 곱처럼 연산되는 것이 아닌, 행렬의 위치에 따라 연산이 된다는 것이 유의점이라고 생각이 들었음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8040f-2fdc-45d5-818f-ea5edbdf359d",
   "metadata": {},
   "source": [
    "<h1>g_tensor_operations_mm.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9bf3fe9-ae02-42c2-94d1-ae15d64031f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9e80dfb-7375-472d-b898-dcc157b0bd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([]) # matmul은 행렬 곱셈을 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b04a223f-974a-40b9-a811-eecb62fc08cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2797aae-5e0e-475a-8119-b564a85897c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4) # 정규 분포에서 랜덤한 3차원 텐서를 생성함\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "749aaa2f-5a18-4cc1-9f58-19cf3fc0f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38640947-801b-4d7e-8ca9-c1bb865479c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654832ed-b6de-4c95-98f8-d6f4ba7ba9a0",
   "metadata": {},
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f94f28ee-77e4-4cef-9c3d-d398ef8c849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34bd8ef1-136a-479b-90ad-8b55b10e78ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4) # broadcasting이 되고 있고 있음, 작은 것을 기준으로 큰 것에 맞추어서 행렬이 확장됨\n",
    "\n",
    "print(\"#\" * 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bae92db-4e0c-4eba-bfa2-b6454846a713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "\n",
    "print(\"#\" * 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc206032-f833-436d-93d3-5a31c8a794ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n"
     ]
    }
   ],
   "source": [
    "def normalize(x): # 이미지 픽셀 값이 0 ~ 255 사이 이므로 그 사이의 값으로 정규화함\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "\n",
    "print(\"#\" * 50, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7318a417-af39-4adf-bcb9-71feceffdb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "# 위 예제 역시 broadcasting이 되며 작은 것을 큰 것에 맞춰짐\n",
    "print(\"#\" * 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4739f34-87a0-4d4a-ae0f-4d237e6dcbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n"
     ]
    }
   ],
   "source": [
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print(\"#\" * 50, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86aeeda7-3531-4ebe-aa4b-0cea6bbf6b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n"
     ]
    }
   ],
   "source": [
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ab6bdc6-6a90-426b-b216-43cc364c0a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2) # 텐서에 제곱 연산을 함\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.]) # 값 범위 내 연속적인 실수 값을 가진 텐서를 생성\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp) # a의 요소를 exp 요소로 거듭제곱 연산을 함\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c31260-9328-4035-ae4e-4cba0123c194",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "텐서의 각종 행렬 연산과 broadcasting에 대해서 학습함\n",
    "차원이 자동으로 맞춰지는 개념과, 텐서의 차원에 대한 개념을 학습하였지만\n",
    "그 두개가 혼용되니 헷갈리는 부분이 많아서 어려움이 있었지만 예제를 여러 번을 실행해보면서 학습할 수 있었음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd2b80-8c5f-4f43-87ab-f9ebdaef981d",
   "metadata": {},
   "source": [
    "<h1>j_tensor_indexing_slicing.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7410c03f-cdc0-4264-b873-7102e9ba4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5258e2b-d45e-4c52-a9ff-4fbc05a1fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "# 텐서에서도 파이썬의 인덱싱과 슬라이싱을 활용할 수 있음\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaafbbed-ec97-47e0-a1dc-0dc62577ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n"
     ]
    }
   ],
   "source": [
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])\n",
    "\n",
    "print(\"#\" * 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d852bc7f-042c-43f6-ad74-98396f8f8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1 # 텐서의 2열의 1번부터 3번 행 값을 1로 채움\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4]) # 1번부터 3번행, 1번부터 3번열 부분을 슬라이싱함\n",
    "\n",
    "print(\"#\" * 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99fdf69c-5f7b-4ce5-9d80-5aeb72cb646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb2e4135-8e82-4125-aa23-3d6a634eb7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0fbf0c-4b96-4c8d-8558-73f479efa169",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "텐서에서도 인덱싱과 슬라이싱이 적용되는 것을 알 수 있었음\n",
    "파이썬이 능숙하지 않아서, 슬라이싱에 대해서 어색한 점이 있었지만\n",
    "예제를 실행해보면서 감을 잡을 수 있었음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f7420-36fc-4b13-84fd-2bb079e13137",
   "metadata": {},
   "source": [
    "<h1>k_tensor_reshaping.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22bf2842-8250-45a6-8781-bb5cbeadb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0e2c6ec-a4ab-452a-addc-a893746e88a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2) # 텐서의 모양을 변경함, 연속적인 텐서에만 사용 가능하며 참조를 사용함\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6) #텐서의 모양을 변경하지만, 불연속적인 텐서일 경우 참조가 아닌 복사를 이용함\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ed499de-be80-44bc-91e7-358c40804d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4) # arange로 1차원 텐서 생성 후 텐서 모양을 변경함\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bc6b29a-a902-489a-9751-97abfaa324c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,) # 크기가 1인 차원을 모두 제거함\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1) # 함수 인자인 0번 차원 값을 제거함\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd453a75-ef82-4207-b466-301ac29e0d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1) # 함수 인자인 1번 위치에 새로운 차원을 추가함\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3) # (2, 3)에서 1번 위치에 차원을 추가하여 (2, 1, 3) 이 됨\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7e67b56-bdb5-44e0-b340-0d5182048815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,) # 다차원 텐서를 1차원 텐서로 만듬\n",
    "\n",
    "print(t14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c29274ba-e16a-4b4b-99cc-8223a599679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1) # start_dim의 값 부터 평탄화를 함 \n",
    "#(2, 2, 2) 텐서가 (2, 4)로 바뀜\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8e1e282-9327-4229-a367-78d548020960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "# 텐서의 순서를 새롭게 지정함 아래에서는 2번->0번, 0번->1번, 1번->2번이 됨\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e594d475-fa95-4d76-bae6-b10af3b18353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "# 차원의 순서를 바꿈\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2) # 텐서의 두 차원을 교환함\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2) # 2차원 텐서의 행과 열을 바꿈 transpose를 2차원에서 실행한 것과 동일\n",
    "\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3b155-6155-46cc-ae69-b74284fd1c4d",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "텐서의 shape를 바꾸는 각종 함수에 대해서 학습함\n",
    "squeeze가 인자가 없으면 사이즈가 1인 차원을 삭제하고, 인자를 주어지면 인자의 차원이 지워진다는 것에 유의해야할 것 같음\n",
    "차원의 shape가 데이터는 유지한 채 변하고, 펼쳐지고 순서가 바뀌는 함수가 많은데 이때 어떤 형태로 shape이 되어있고\n",
    "데이터가 나열되어있을까라는 생각을 많이 해야할 것 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c68117-824e-4f9f-a16d-b1196d3ebfef",
   "metadata": {},
   "source": [
    "<h1>l_tensor_concat.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f65b9ae0-ed2d-4349-ba1d-91a0e673c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6665e9c-7bd9-41e6-a06c-db1dc06621fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1) # 여러 텐서를 dim으로 선택한 차원에서 연결하여 하나의 텐서로 만듬, 사이즈만 변하고 차원은 변하지 않음\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aaad602f-af9d-4029-82d9-e306f960ead2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(\"#\" * 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c39e764-5170-418f-a2a3-57558223a8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1744822-41df-435e-90b7-dee9d1bbcfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n"
     ]
    }
   ],
   "source": [
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3b3aeb66-3792-4d3d-8c5e-7f4ff49c0d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca15f5-4522-4f2e-8f10-d854d03c41f2",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "cat은 차원은 변하지 않고, 사이즈만 변한다는 것을 유의해야할 것 같음\n",
    "추가로 탐색하여, concat은 동일한 기능이지만 이름만 다르게 파이토치에서 추후에 나온 함수라는데 나온 이유는\n",
    "함수명의 직관성과 다른 프레임워크, 언어와 동일성이 가져오기 위함이라는 것을 추가적으로 알아봄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07fdc5-eb86-46d5-9bb7-ec19ca37409c",
   "metadata": {},
   "source": [
    "<h1>m_tensor_stacking.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "177fd5ed-125b-4a26-946f-1c6a0cbccc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3242e65d-911e-465a-818e-41f0e5620099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0) # t1과 t2를 dim 번째 차원에 추가하여 합침, stack은 새로운 차원으로 확장하여 병합됨\n",
    "# 결과값으로 (2, 3) -> (2, 2, 3)>이 됨\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "# t1과 t2에 새로운 차원을 unsqueeze로 추가하며, dim 번째에 cat 함수를 이용하여 이어붙임\n",
    "# 2, 2, 3이 되어서 t3와 동일한 모양이 됨\n",
    "print(t3.shape, t3.equal(t4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60d425c8-ec1a-437f-babb-b375425b0862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95951805-45a3-45e2-aeff-0fa0e58c43c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18d123f1-efe6-406a-a15e-00a5039c0c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce1cb9f-5471-4cd4-8ae7-e32674a5326d",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "stack에 대해서 학습을 함\n",
    "stack과 cat의 차이점에 대해서 헷갈리지 않고 알아야 할 것 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b503d3a-037a-4261-9114-a240571bb83c",
   "metadata": {},
   "source": [
    "<h1>n_tensor_vstack_hstack.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5219f024-b8f6-4fed-ab4b-a87d0e4a223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7e1c79e-1ae2-438e-8442-0f544e1c1a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2)) # 각 텐서를 새로운 행으로 추가시킴 => 수직으로 병합함\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1cdf9508-18d9-4586-8460-85f1c3800ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1c8fc496-9b72-4ecf-ada6-3348328d9c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "601ab558-412d-4464-b92c-0af964666afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c37e4777-42e8-40f0-8785-b53b38463626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dd4b3df9-389c-45d8-bf92-07e1da2a3ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11)) # 텐서를 1차원 텐서로 연결함 => hstack은 수평으로 병합함\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a789a2cb-513e-47ed-b850-e47896762e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ddd806a8-858d-49dd-b8f0-c2b632e19d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4ad9c892-2c7d-46f1-9b9c-68f57a3f2027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ddcde-5ca2-426e-949d-63e149dc672e",
   "metadata": {},
   "source": [
    "<h1>취득한 기술 혹은 고찰 사항</h1>\n",
    "\n",
    "vstack과 hstack에 대해서 학습하였으며 함수 명에 적혀있듯이 v와 h의 뜻을 잘 생각하고 헷갈리지 않게 사용해야할 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d7d03-304e-4b21-89ec-74a2ccd84ce2",
   "metadata": {},
   "source": [
    "<h1>숙제 후기</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b079e23b-eb13-4bed-ae8c-9e63451203a3",
   "metadata": {},
   "source": [
    "이전에는 파이썬에 대해 알고 있는 지식이 적을 뿐만 아니라, 파이토치 그리고 텐서에 대한 기본 개념과 함수에 대한 지식이 부족했다.</br>\n",
    "하지만 이번 hw1_1 과제를 수행하면서 기본적인 파이토치와 텐서 그리고 파이썬에 동작에 대해서도 생각해보고 학습할 수 있었다.</br>\n",
    "많은 예제를 접하면서 텐서의 모양을 변형시키고 수정하는 과정에서 같은 코드를 계속 복기해보는 등 어려움을 겪었으며 주피터 사용에도 미숙하여\n",
    "힘든 점이 많았던 것 같다. 하지만 힘든 점을 해결하면서 얻는 점이 많았고 특강의 내용을 복기하면서 과제 진행하고 파이토치 예제를 많이 실행해보고 고찰하니\n",
    "이제 텐서에 대해서 조금 알게 된 것 같아서 유의미했던 시간이었던 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
